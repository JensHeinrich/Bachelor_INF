\documentclass[
german,
]{bachelor}

\input{glossaries/main.tex}
\input{bibliographies/main.tex}

\author{Jens Heinrich}
\title{%
Analyse der Möglichkeit
einer automatischen Anreicherung von sprachwissenschatflichen Texten
durch Named Entity Recognition
unter Verwendung von BERT
mit Semantik aus der BLL Onthologie%
}

\usepackage{amsthm}
\newtheorem{prob}{Problemstellung}

\nocite{stack:tex:argmin}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{csquotes}

\begin{document}

\maketitle

\tableofcontents

\section{Einleitung}

\section{Literaturrecherche}

\section{Grundlagen und Definitionen}
Der Prozess der \gls{namedentityrecognition}
lässt sich nach \autocite{2006.15509} als Lösung des folgenden mathematischen Problems beschreiben:

\begin{prob}{\glspt{namedentityrecognition}}\label{prob:nlp:ner}
Für ein \gls{nlp:sentence} \glsentryformula{nlp:sentence}
wird eine \gls{nlp:label_sequence} \glsentryformula{nlp:label_sequence} gesucht,
sodass \(
\glssymbol{nlp:label}_i \in
\left\lbrace
	\glssymbol{nlp:label:beginning}\texttt{-X},
	\glssymbol{nlp:label:inside}\texttt{-X},
	\glssymbol{nlp:label:outside}
	:
	\text{\texttt{X} ist Entitätstyp}
\right\rbrace
\)
und gilt:
\[
	\glssymbol{nlp:label}_i =
	\begin{cases}
		\texttt{\glssymbol{nlp:label:beginning}-X},
		\text{wenn \(\glssymbol{nlp:token}_i\)
		der Beginn einer Entität vom Typ \(\texttt{X}\) ist
	}\\
	\texttt{\glssymbol{nlp:label:inside}-X},
		\text{wenn \(\glssymbol{nlp:token}_i\)
		ein Token einer Entität vom Typ \(\texttt{X}\)
		und nicht deren Beginn ist
	}\\
	\glssymbol{nlp:label:outside}, \text{sonst}.
\end{cases}
\]
Diese \texttt{BIO}-Klassifikation geht auf \citeauthor{10.1145/2396761.2398506} zurück \autocite{10.1145/2396761.2398506}.
\end{prob}

Seien \(M\) bereits annotierte \glslink{nlp:sentence}{Sätze}
\glsentryformula{nlp:labelled_sentences} gegeben
und beschreibe \glsentryformula{nlp:model} ein \gls{namedentityrecognition}[-Modell],
wobei \glssymbol{nlp:sentence} ein Satz
und \glssymbol{nlp:model:parameters} die Parameter des Modells sind.
Dann lässt sich anhand der Problemstellung~\cref{prob:nlp:ner}
die Güte des Modells über eine Metrik bestimmen.
Eine beliebte Metrik im Rahmen des \gls{machine-learning} ist die \gls{crossentropyloss}[-Funktion],
wie sie in \autocite[5.5]{juraksky2000speech} definiert wird:
\glsentryformula{crossentropyloss}.
Hierbei wäre der ideale Fall,
dass jeder Satz
\gls{nlp:label} entsprechend der vorgegeben (idealen) Annotation
zugewiesen bekommt.
Dementsprechend werden die \gls{nlp:model:parameters} so gesucht,
dass sie diese Distanz über alle Trainingsdaten minimieren.
Somit sind die idealen Parameter durch die Formel
\autocite[1]{2006.15509}
gegeben:
\begin{equation}
	\hat{\glssymbol{nlp:model:parameters}} : =
	\argmin_{\glssymbol{nlp:model:parameters}}
	\frac{1}{M}
	\sum_{m=1}^{M}
	\glssymbol{crossentropyloss}\left(
		\glssymbol{nlp:label_sequence}_m,
		\glssymbol{nlp:model}\left(
				\glssymbol{nlp:sentence}_m;
				\glssymbol{nlp:model:parameters}
			\right)
		\right)
\end{equation}

\subsection{fernüberwachte \glspt{namedentityrecognition}}
Hierbei sind im Gegensatz zur \enquote{klassischen} \gls{namedentityrecognition}
% TODO das klingt unrund
nicht die \gls{nlp:token} selber beschriftet,
sondern der gesamte Text wird automatisch mit \glspl{label} versehen.
\citeauthor{2006.15509} listet als Möglichkeiten hierfür
\enquote{string matching}, \enquote{regexp} und heuristische Verfahren
\autocite{2006.15509}.

\section{Vorherige Arbeiten}


\section{Methodik}

\subsection{Vorbereitung der Beispieldokumente}
Da die Beispieldokumente als \gls{pdf} vorliegen,
müssen die eigentlichen Texte extrahiert werden.
Hierfür gibt es verschiedenen Werkzeuge und Bibliotheken,
wie z.B.\, \gls{pypdf2} oder \gls{pdftotext}.

Der erste Versuch der Extraktion wird mit \gls{pypdf2} durchgeführt.
Dieses analysiert die \gls{pdf} seitenweise.
Der kommentierte Code findet sich in \texttt{src/extract\_fulltexts.ipynb}.

\subsection{\glspt{naturallanguageprocessing} mit \glspt{BERT}}
Der Prozess des \gls{naturallanguageprocessing} mit \gls{BERT} besteht aus verschiedenen Schritten.
Zuerst wird der Text in Tokens aufgeteilt,
welche in numerische Werte übersetzt werden können.
Diese numerischen Werte können durch das neuronale Netzwerk verarbeitet werden,
da dieses aber eine Eingabe fixer Länge erwartet,
müssen zu lange Sequenzen abgeschnitten (\enquote{truncate})
und zu kurze Sequenzen ergänzt (\enquote{pad}) werden.
Die so bearbeiteten Werte werden dann in einen \gls{tensor} umgewandelt
mit dem die weiteren Berechnungen durchgeführt werden können.
Diese Schritte wird von einem \enquote{preprocessor}
bzw.\, \enquote{tokenizer} ausgeführt.
\autocite{huggingface:docs:Transformers:preprocessing}
Eine beliebte Bibliothek hierfür ist \gls{huggingface:tokenizers}.



Der darauf folgende Schritt ist die Auswertung durch


\section{Diskussion}

\section{Fazit}

\end{document}
