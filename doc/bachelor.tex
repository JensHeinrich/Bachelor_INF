\documentclass[
german,
]{bachelor}

\input{glossaries/main.tex}
\input{bibliographies/main.tex}

\author{Jens Heinrich}
\title{%
Analyse der Möglichkeit
einer automatischen Anreicherung von sprachwissenschatflichen Texten
durch Named Entity Recognition
unter Verwendung von BERT
mit Semantik aus der BLL Onthologie%
}

\begin{document}

\maketitle

\tableofcontents

\section{Einleitung}

\section{Literaturrecherche}

\section{Grundlagen und Definitionen}
\begin{prob}{\glspt{namedentityrecognition}}
Der Prozess der \gls{namedentityrecognition}
lässt sich nach \autocite{Liang_2020} als folgendes mathematisches Problem beschreiben:

Für ein \gls{nlp:sentence} \glsentryformula{nlp:sentence}
wird eine \gls{nlp:label_sequence} \glsentryformula{nlp:label_sequence} gesucht,
sodass \(
\glssymbol{nlp:label}_i \in
\left\lbrace
	\glssymbol{nlp:token:beginning}\texttt{-X},
	\glssymbol{nlp:token:inside}\texttt{-X},
	\glssymbol{nlp:token:outside}
	:
	\text{\texttt{X} ist Entitätstyp}
\right\rbrace
\)
und gilt:
\[
	\glssymbol{nlp:label}_i =
	\begin{cases}
		\texttt{\glssymbol{nlp:label:beginning}-X},
		\text{wenn \(\glssymbol{nlp:token}_i\)
		der Beginn einer Entität vom Typ \(\texttt{X}\) ist
	}\\
	\texttt{\glssymbol{nlp:label:inside}-X},
		\text{wenn \(\glssymbol{nlp:token}_i\)
		ein Token einer Entität vom Typ \(\texttt{X}\)
		und nicht deren Beginn ist
	}\\
	\glssymbol{nlp:label:outside}, \text{sonst}.
\end{cases}
\]
Diese \texttt{BIO}-Klassifikation geht auf \citeauthor{10.1145/2396761.2398506} zurück \autocite{10.1145/2396761.2398506}.
\end{prob}



\section{Methodik}

\subsection{Vorbereitung der Beispieldokumente}
Da die Beispieldokumente als \gls{pdf} vorliegen,
müssen die eigentlichen Texte extrahiert werden.
Hierfür gibt es verschiedenen Werkzeuge und Bibliotheken,
wie z.B.\, \gls{pypdf2} oder \gls{pdftotext}.

Der erste Versuch der Extraktion wird mit \gls{pypdf2} durchgeführt.
Dieses analysiert die \gls{pdf} seitenweise.
Der kommentierte Code findet sich in \texttt{src/extract\_fulltexts.ipynb}.

\subsection{\glspt{naturallanguageprocessing} mit \glspt{BERT}}
Der Prozess des \gls{naturallanguageprocessing} mit \gls{BERT} besteht aus verschiedenen Schritten.
Zuerst wird der Text in Tokens aufgeteilt,
welche in numerische Werte übersetzt werden können.
Diese numerischen Werte können durch das neuronale Netzwerk verarbeitet werden,
da dieses aber eine Eingabe fixer Länge erwartet,
müssen zu lange Sequenzen abgeschnitten (\enquote{truncate})
und zu kurze Sequenzen ergänzt (\enquote{pad}) werden.
Die so bearbeiteten Werte werden dann in einen \gls{tensor} umgewandelt
mit dem die weiteren Berechnungen durchgeführt werden können.
Diese Schritte wird von einem \enquote{preprocessor}
bzw.\, \enquote{tokenizer} ausgeführt.
\autocite{huggingface:docs:Transformers:preprocessing}
Eine beliebte Bibliothek hierfür ist \gls{huggingface:tokenizers}.



Der darauf folgende Schritt ist die Auswertung durch


\section{Diskussion}

\section{Fazit}

\end{document}
