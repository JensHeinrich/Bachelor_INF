@Acronym{BERT,
	short = {BERT},
	long = {Bidirectional Encoder Representations from Transformers},
}

@Acronym{namedentityrecognition,
	short = {NER},
	long = {Named Entity Recognition},
	parent = {naturallanguageprocessing},
}

@Acronym{naturallanguageprocessing,
	short = {NLP},
	long = {Natural Language Processing},
}

@entry{tensor,
	name = {Tensor},
	description-de = {Erweiterung des Konzeptes eines Vektors ins mehrdimensionale},
}

@entry{nlp:token,
	name = {Token},
	symbol = {\ensuremath{x}},
}

@entry{nlp:sentence,
	name-de = {Satz},
	plural-de = {S채tze},
	symbol = {\ensuremath{\mathbf{X}}},
	description-de = {Liste aus \(N\) Tokens \glsentryformula{nlp:sentence}},
	formula = {
	    \ensuremath{
	    \glssymbol{nlp:sentence}
	    = \left\lbrack
		   \glssymbol{nlp:token}_1,
		   \hdots,
		   \glssymbol{nlp:token}_N
	    \right\rbrack
	    }
},
}

@entry{nlp:entity,
	name-de = {Entit채t},
	description-de = {F체r einen \gls{nlp:sentence} \glsentryformula{nlp:sentence} ist eine Entit채t ein Bereich (\enquote{span}) von Tokens \glsentryformula{nlp:entity}},
	formula = {\ensuremath{s=\left\lbrack x_i, \hdots, x_j \right\rbrack \left(0\leq i \leq j \leq N \right) }},
}

@entry{nlp:ner,
	description-de = {Zuweisen von einer Liste von \glspl{nlp:label} \glsentryformula{nlp:label_sequence} zu einem Satz \glssymbol{nlp:sentence}},
}

@entry{nlp:label,
	name = {Label},
	symbol = {\ensuremath{y}},
}

@entry{nlp:label_sequence,
	name = {Beschriftung},
	symbol = {\ensuremath{\mathbf{Y}}},
	formula = {\ensuremath{\glssymbol{nlp:label_sequence} = \left\lbrack \glssymbol{nlp:label}_1, \hdots, \glssymbol{nlp:label}_N \right\rbrack}},
}

@entry{nlp:label:beginning,
	symbol = {\ensuremath{\texttt{B}}},
}

@entry{nlp:label:inside,
	symbol = {\ensuremath{\texttt{I}}},
}
@entry{nlp:label:outside,
	symbol = {\ensuremath{\texttt{O}}},
}

@entry{nlp:labelled_sentences,
	name-de = {Beschrifte \glspp{nlp:sentence}},
	formula = {
	    \ensuremath{
		{
			\left\lbrace
				\glssymbol{nlp:sentence}_m,
				\glssymbol{nlp:label_sequence}_m
			\right\rbrace
		}_{m=1}^M
		}
	},
}

@entry{nlp:model:parameters,
	name-de = {Parameter},
	description-de = { Parameter des Models},
	symbol = {\ensuremath{\Theta}},
	parent = {nlp:model},
}

@entry{nlp:model,
	symbol = { \ensuremath{f}},
	formula = {
	    \ensuremath{
	    \glssymbol{nlp:model}
	    \left(
	    \glssymbol{nlp:sentence};
	    \glssymbol{nlp:model:parameters}
	    \right)
	    }
	},
	name-de = {\glspt{naturallanguageprocessing}-Modell},
}

@entry{crossentropy,
	name-de = {Kreuzentropie},
	name-en = {Crossentropy},
	description-de = {},
}

@entry{crossentropyloss,
	name-de = {\glspt{crossentropy}-Kosten},
	description-de = {Differenz zwischen der erwarteten Verteilung \(\hat{y}\) und der echten Verteilung \(y\) },
	symbol = {\ensuremath{l}},
	formula = {\ensuremath{
	    \glssymbol{crossentropyloss}\left(y,\hat{y}\right)
	    =
	    - \left\lbrack
	    y \log \hat{y}
	    + \left(1-y\right) \log \left( 1-\hat{y} \right)
	    \right\rbrack
	    }},
}
